# Backpropagation


Backprop is the algorithm for determining how a single training example would like to nudge the weights and biases.

Gradient descent (GD) is computationally slow, so we use SGD (mini-batches).

GD need a lot data to work (ehy)

